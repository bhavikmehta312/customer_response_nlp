{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2a32af",
   "metadata": {},
   "source": [
    "# Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45b2831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gpt3_tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4f610",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4988d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "file_path = 'hf://datasets/Kaludi/Customer-Support-Responses/Customer-Support.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8083f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9ee76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0                       My order hasn't arrived yet.   \n",
      "1                      I received a damaged product.   \n",
      "2                          I need to return an item.   \n",
      "3              I want to change my shipping address.   \n",
      "4                   I have a question about my bill.   \n",
      "..                                               ...   \n",
      "69  How do I schedule a consultation or appointment?   \n",
      "70                   Can I get a copy of my receipt?   \n",
      "71    Can I use a competitor's coupon at your store?   \n",
      "72                  Do you have a recycling program?   \n",
      "73       How do I report a lost or stolen gift card?   \n",
      "\n",
      "                                             response  \n",
      "0   We apologize for the inconvenience. Can you pl...  \n",
      "1   We apologize for the inconvenience. Can you pl...  \n",
      "2   Certainly. Please provide your order number an...  \n",
      "3   No problem. Can you please provide your order ...  \n",
      "4   We'd be happy to help. Can you please provide ...  \n",
      "..                                                ...  \n",
      "69  We'd be happy to help. Can you please provide ...  \n",
      "70  Certainly. Can you please provide your order n...  \n",
      "71  In some cases, we may accept competitor coupon...  \n",
      "72  Yes, we do have a recycling program. Can you p...  \n",
      "73  We're sorry to hear that. Can you please provi...  \n",
      "\n",
      "[74 rows x 2 columns]\n",
      "[15496, 11, 703, 389, 345, 30]\n"
     ]
    }
   ],
   "source": [
    "# Display some basic information about the dataset\n",
    "print(df)\n",
    "\n",
    "# Define a function to tokenize the dialogues\n",
    "def gpt3_tokenize(text):\n",
    "    tokens = gpt3_tokenizer.encode(text)\n",
    "    return tokens  # Return token IDs\n",
    "\n",
    "# Test the tokenizer function\n",
    "example_text = \"Hello, how are you?\"\n",
    "tokens = gpt3_tokenize(example_text)\n",
    "print(tokens)\n",
    "\n",
    "text = \" \".join(review for review in df['response'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "# Save the preprocessed responses as a JSON file\n",
    "with open('data/response_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)\n",
    "\n",
    "text = \" \".join(review for review in df['query'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "# Save the preprocessed query as a JSON file\n",
    "with open('data/query_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fe53a",
   "metadata": {},
   "source": [
    "# Data Preprocessing and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "273ecffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['query'] = df['query'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)\n",
    "\n",
    "# Adding special tokens for clarity\n",
    "df['text'] = df['query'] + \" <|endoftext|> \" + df['response']\n",
    "\n",
    "# training and validation dataset, keeping 20% validation dataset\n",
    "train_df, val_df = train_test_split(df[['text']], test_size=0.2, random_state=42)\n",
    "train_texts = train_df['text'].tolist()\n",
    "val_texts = val_df['text'].tolist()\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437220c",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = CustomerDataset(train_encodings)\n",
    "val_dataset = CustomerDataset(val_encodings)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d4b0f",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a013dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.4876656532287598\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss']))}\")\n",
    "\n",
    "# Function to generate responses with attention mask and adjusted parameters\n",
    "def generate_response(model, tokenizer, query, max_length=128, max_new_tokens=50, temperature=0.7, top_k=50, top_p=0.9, do_sample=True):\n",
    "    inputs = tokenizer.encode_plus(query + \" <|endoftext|>\", return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split('<|endoftext|>')[-1].strip().replace(query,\"\")\n",
    "\n",
    "# Generate responses for some sample queries\n",
    "sample_queries = [\n",
    "    \"i have a question about my bill.\",\n",
    "    \"can i get a replacement part for my product?\",\n",
    "    \"can i place a bulk order?\",\n",
    "    \"my order hasn't arrived yet.\",\n",
    "    \"i can't log into my account.\",\n",
    "    \"How do I report a problem with your website?\",\n",
    "    \"I am not able to access your website\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    print(f\"Query: {query}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44a5366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('customer_support_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('customer_support_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a04a135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\BHAVIK\\\\OneDrive - Rising Analytics\\\\Desktop\\\\practice\\\\freelancer\\\\hugging_face'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d450ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(model, tokenizer, query, max_length=128, max_new_tokens=50, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    input_text = query + \" \"\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True  # Enable sampling mode\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_parts = response.split(\" \")\n",
    "    if len(response_parts) > 1:\n",
    "        return response_parts[1].strip()  # Extract the response part\n",
    "    else:\n",
    "        return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8d5198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42998a8398b94d4f9d11165b526a92d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Query:', layout=Layout(height='100px', width='100%'), placeholder='Type your q…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae55e5b51f34f6b87cee8b0225c1d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Get Response', icon='check', style=ButtonStyle(), tooltip='Click to g…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df7989382634a4dbc2ba0c78e9b31c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='100px', width='100%'), placeholder='Response will appe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mon_button_click\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_button_click\u001b[39m(b):\n\u001b[0;32m     10\u001b[0m     query \u001b[38;5;241m=\u001b[39m query_input\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m---> 11\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     response_output\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(model, tokenizer, query, max_length, max_new_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(model, tokenizer, query, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m):\n\u001b[0;32m      7\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m query \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:3118\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3098\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[0;32m   3099\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3114\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[0;32m   3115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 3118\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3119\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3120\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   3121\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3122\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3123\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3125\u001b[0m )\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3128\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3129\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3147\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2849\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2847\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m   2848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 2849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2853\u001b[0m     )\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[0;32m   2856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2857\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[0;32m   2858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2861\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2862\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "# Jupyter notebook cell to create a simple demo\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the function to handle the query and generate response\n",
    "def on_button_click(b):\n",
    "    query = query_input.value\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    response_output.value = f\"Response: {response}\"\n",
    "\n",
    "# Create input and output widgets\n",
    "query_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your query here',\n",
    "    description='Query:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "response_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Response will appear here',\n",
    "    description='',\n",
    "    disabled=True,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(\n",
    "    description='Get Response',\n",
    "    disabled=False,\n",
    "    button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to get response',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Assign the button click event\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(query_input, button, response_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a10951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
