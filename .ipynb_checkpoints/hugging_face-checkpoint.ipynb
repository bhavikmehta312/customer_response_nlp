{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c72067e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a3f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BHAVIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BHAVIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "import gpt3_tokenizer\n",
    "import nltk\n",
    "import os\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sentencepiece\n",
    "\n",
    "os.chdir(\"C:/Users/BHAVIK/OneDrive - Rising Analytics/Desktop/practice/freelancer/hugging_face\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Kaludi/Customer-Support-Responses/Customer-Support.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6649b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a02e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5493f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0                       My order hasn't arrived yet.   \n",
      "1                      I received a damaged product.   \n",
      "2                          I need to return an item.   \n",
      "3              I want to change my shipping address.   \n",
      "4                   I have a question about my bill.   \n",
      "..                                               ...   \n",
      "69  How do I schedule a consultation or appointment?   \n",
      "70                   Can I get a copy of my receipt?   \n",
      "71    Can I use a competitor's coupon at your store?   \n",
      "72                  Do you have a recycling program?   \n",
      "73       How do I report a lost or stolen gift card?   \n",
      "\n",
      "                                             response  \n",
      "0   We apologize for the inconvenience. Can you pl...  \n",
      "1   We apologize for the inconvenience. Can you pl...  \n",
      "2   Certainly. Please provide your order number an...  \n",
      "3   No problem. Can you please provide your order ...  \n",
      "4   We'd be happy to help. Can you please provide ...  \n",
      "..                                                ...  \n",
      "69  We'd be happy to help. Can you please provide ...  \n",
      "70  Certainly. Can you please provide your order n...  \n",
      "71  In some cases, we may accept competitor coupon...  \n",
      "72  Yes, we do have a recycling program. Can you p...  \n",
      "73  We're sorry to hear that. Can you please provi...  \n",
      "\n",
      "[74 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display some basic information about the dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db596e1",
   "metadata": {},
   "source": [
    "# Tokenizing the dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e6b0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 703, 389, 345, 30]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to tokenize the dialogues\n",
    "def gpt3_tokenize(text):\n",
    "    tokens = gpt3_tokenizer.encode(text)\n",
    "    return tokens  # Return token IDs\n",
    "\n",
    "# Test the tokenizer function\n",
    "example_text = \"Hello, how are you?\"\n",
    "tokens = gpt3_tokenize(example_text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa260a",
   "metadata": {},
   "source": [
    "## Storing the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d8306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in df['response'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "# Save the preprocessed responses as a JSON file\n",
    "with open('data/response_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)\n",
    "\n",
    "text = \" \".join(review for review in df['query'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "# Save the preprocessed query as a JSON file\n",
    "with open('data/query_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8abd760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   query  \\\n",
      "0           My order hasn't arrived yet.   \n",
      "1          I received a damaged product.   \n",
      "2              I need to return an item.   \n",
      "3  I want to change my shipping address.   \n",
      "4       I have a question about my bill.   \n",
      "\n",
      "                                            response  \n",
      "0  We apologize for the inconvenience. Can you pl...  \n",
      "1  We apologize for the inconvenience. Can you pl...  \n",
      "2  Certainly. Please provide your order number an...  \n",
      "3  No problem. Can you please provide your order ...  \n",
      "4  We'd be happy to help. Can you please provide ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db428e2329b4448cb8f42e51d3d6041a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the data\n",
    "print(df.head())\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Tokenize the response\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b890d2",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efb4e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\bhavik\\appdata\\roaming\\python\\python39\\site-packages (4.42.4)\n",
      "Requirement already satisfied: torch in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\bhavik\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bhavik\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db08f0",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84d24438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   query  \\\n",
      "0           My order hasn't arrived yet.   \n",
      "1          I received a damaged product.   \n",
      "2              I need to return an item.   \n",
      "3  I want to change my shipping address.   \n",
      "4       I have a question about my bill.   \n",
      "\n",
      "                                            response  \n",
      "0  We apologize for the inconvenience. Can you pl...  \n",
      "1  We apologize for the inconvenience. Can you pl...  \n",
      "2  Certainly. Please provide your order number an...  \n",
      "3  No problem. Can you please provide your order ...  \n",
      "4  We'd be happy to help. Can you please provide ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b1476413842399452b21a79622e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sacrebleu\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv(\"hf://datasets/Kaludi/Customer-Support-Responses/Customer-Support.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the data\n",
    "print(df.head())\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Tokenize the response\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5e17c",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97bf0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a452f",
   "metadata": {},
   "source": [
    "# Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93f9cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3742e1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f651eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Loss: 0.956267237663269\n",
      "Epoch 2/3 completed. Loss: 0.6769994497299194\n",
      "Epoch 3/3 completed. Loss: 0.7321780920028687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('t5_seq2seq_tokenizer\\\\tokenizer_config.json',\n",
       " 't5_seq2seq_tokenizer\\\\special_tokens_map.json',\n",
       " 't5_seq2seq_tokenizer\\\\spiece.model',\n",
       " 't5_seq2seq_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {loss.item()}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"t5_seq2seq_model\")\n",
    "tokenizer.save_pretrained(\"t5_seq2seq_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62d1f8",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73ea07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: question: Is my payment information secure?\n",
      "Response: Yes, we take security very seriously. Can you please provide your account email so we can verify the security measures in place for your payment information?\n",
      "Generated: <pad> secure <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "==================================================\n",
      "Query: question: Can I change the size of an item in my order?\n",
      "Response: Certainly. Can you please provide your order number and the details of the item you'd like to change?\n",
      "Generated: <pad> change the size of an item in my order <pad>\n",
      "==================================================\n",
      "Query: question: Can I get a replacement part for my product?\n",
      "Response: Certainly. Can you please provide the product name, SKU, or serial number, and a description of the part you need?\n",
      "Generated: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "==================================================\n",
      "Query: question: Are there any current promotions or sales?\n",
      "Response: We'd be happy to inform you of any current promotions. Can you please provide the product name or SKU you're interested in, or the type of promotion you're looking for?\n",
      "Generated: <pad> promotions or sales <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "==================================================\n",
      "Query: question: I can't log into my account.\n",
      "Response: We apologize for the inconvenience. Can you please provide your account email so we can help you troubleshoot the issue?\n",
      "Generated: <pad> I can't log into my account <pad> <pad>\n",
      "==================================================\n",
      "Query: question: Do you offer financing options?\n",
      "Response: We do offer financing options for select purchases. Can you please provide the product name or SKU and your email address so we can send you more information?\n",
      "Generated: <pad> offer financing options? <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "==================================================\n",
      "Query: question: My order hasn't arrived yet.\n",
      "Response: We apologize for the inconvenience. Can you please provide your order number so we can investigate?\n",
      "Generated: <pad> my order hasn't arrived yet <pad> <pad>\n",
      "==================================================\n",
      "Query: question: Can I use multiple promo codes on one order?\n",
      "Response: In most cases, only one promo code can be applied per order. Can you please provide the promo codes you're trying to use so we can check their compatibility?\n",
      "Generated: <pad> can I use multiple promo codes on one order?\n",
      "==================================================\n",
      "BLEU Score: 1.1761153189349267\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "metric = load_metric(\"sacrebleu\", trust_remote_code=True)\n",
    "\n",
    "for batch in eval_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=128)\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "\n",
    "    metric.add_batch(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "\n",
    "    for i in range(len(decoded_preds)):\n",
    "        print(f\"Query: {tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)}\")\n",
    "        print(f\"Response: {decoded_labels[i]}\")\n",
    "        print(f\"Generated: {decoded_preds[i]}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Compute the BLEU score\n",
    "final_score = metric.compute()\n",
    "print(f\"BLEU Score: {final_score['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412e471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8762717150845322,\n",
       " 'counts': [17, 3, 0, 0],\n",
       " 'totals': [182, 174, 166, 158],\n",
       " 'precisions': [9.340659340659341,\n",
       "  1.7241379310344827,\n",
       "  0.30120481927710846,\n",
       "  0.15822784810126583],\n",
       " 'bp': 0.936192589240614,\n",
       " 'sys_len': 182,\n",
       " 'ref_len': 194}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1f628",
   "metadata": {},
   "source": [
    "# Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a91c21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BLEU metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_score['score']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16f9be",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59225f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e3f62",
   "metadata": {},
   "source": [
    "# Using more advanced model checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe20b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471677</td>\n",
       "      <td>1.289398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.456002</td>\n",
       "      <td>2.471341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.450491</td>\n",
       "      <td>3.756404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=0.523827729401765, metrics={'train_runtime': 107.0603, 'train_samples_per_second': 1.849, 'train_steps_per_second': 0.252, 'total_flos': 6699419172864.0, 'train_loss': 0.523827729401765, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fa2bf",
   "metadata": {},
   "source": [
    "# Evaluate the generated responses for quality and appropriateness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a952bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 3.756403588174499\n",
      "Input: can I get a copy of my receipt?\n",
      "Output: <pad> Kann ich eine copie meines Empfangs erheben?\n",
      "\n",
      "Input: What's the time?\n",
      "Output: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the BLEU score\n",
    "print(\"BLEU score:\", results['eval_bleu'])\n",
    "\n",
    "# Generate some sample responses\n",
    "sample_inputs = [\"can I get a copy of my receipt?\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "input_ids = tokenizer(sample_inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "output_ids = model.generate(input_ids)\n",
    "sample_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Input: {sample_inputs[i]}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f349075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb8069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c272280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb93d0a70dab4c23b9ee99514824060c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.777867</td>\n",
       "      <td>1.598692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721144</td>\n",
       "      <td>1.728882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702676</td>\n",
       "      <td>1.771053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.771053122412459\n",
      "Input: How do you feel today?\n",
      "Output: <pad> How do you feel today? <pad>\n",
      "\n",
      "Input: What's the time?\n",
      "Output: <pad> Was ist der Zeitpunkt? <pad> <pad>\n",
      "\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> Can you tell me something interesting?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Sample data\n",
    "#data = {\n",
    "#    'query': [\"How are you?\", \"What is the weather today?\", \"Tell me a joke.\"],\n",
    "#    'response': [\"I'm fine, thank you!\", \"It's sunny today.\", \"Why don't scientists trust atoms? Because they make up everything!\"]\n",
    "#}\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Tokenize the response\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n",
    "\n",
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Define the BLEU metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_score['score']}\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the BLEU score\n",
    "print(\"BLEU score:\", results['eval_bleu'])\n",
    "\n",
    "# Generate some sample responses\n",
    "sample_inputs = [\"How do you feel today?\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "input_ids = tokenizer(sample_inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "sample_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Input: {sample_inputs[i]}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a62baa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc846b565e7437c8d33fb393602685d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.777867</td>\n",
       "      <td>1.598692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721144</td>\n",
       "      <td>1.728882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702676</td>\n",
       "      <td>1.771053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.771053122412459\n",
      "Input: How do you feel today?\n",
      "Output: <pad> How do you feel today? <pad>\n",
      "Input: What's the time?\n",
      "Output: <pad> Was ist der Zeitpunkt? <pad> <pad>\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> Can you tell me something interesting?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n",
    "\n",
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Define the BLEU metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Compute BLEU score function\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_score['score']}\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the BLEU score\n",
    "print(\"BLEU score:\", results['eval_bleu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219b8cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My order hasn't arrived yet.\n",
      "Output: <pad> Meine Bestellung ist noch nicht angekommen.\n",
      "Input: What's the time?\n",
      "Output: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate sample responses\n",
    "sample_inputs = [\"My order hasn't arrived yet.\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "input_ids = tokenizer(sample_inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "sample_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Input: {sample_inputs[i]}\")\n",
    "    print(f\"Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa234014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f9124d18dc49e0aa24a7b8fd1b4a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.777867</td>\n",
       "      <td>1.598692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721144</td>\n",
       "      <td>1.728882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702676</td>\n",
       "      <td>1.771053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.771053122412459\n",
      "Input: How do you feel today?\n",
      "Output: <pad> How do you feel today? <pad>\n",
      "Input: What's the time?\n",
      "Output: <pad> Was ist der Zeitpunkt? <pad> <pad>\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> Can you tell me something interesting?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n",
    "\n",
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Define the BLEU metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "# Compute BLEU score function\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_score['score']}\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the BLEU score\n",
    "print(\"BLEU score:\", results['eval_bleu'])\n",
    "\n",
    "# Generate sample responses\n",
    "sample_inputs = [\"How do you feel today?\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "input_ids = tokenizer(sample_inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "sample_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Input: {sample_inputs[i]}\")\n",
    "    print(f\"Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7acc323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f363da767d544f8bb8841f9ef60669a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.421880</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.366894</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.277966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BHAVIK\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0\n",
      "Input: How do you feel today?\n",
      "Output: <pad> Comment vous soyez- <pad>\n",
      "Input: What's the time?\n",
      "Output: <pad> Was ist der Zeitpunkt? <pad> <pad>\n",
      "Input: Can you tell me something interesting?\n",
      "Output: <pad> Can you tell me something interesting?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'query': [\"How are you?\", \"What is the weather today?\", \"Tell me a joke.\"],\n",
    "    'response': [\"I'm fine, thank you!\", \"It's sunny today.\", \"Why don't scientists trust atoms? Because they make up everything!\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Respond to the question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# BLEU metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_score['score']}\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"BLEU score:\", results['eval_bleu'])\n",
    "\n",
    "# Generate sample responses\n",
    "sample_inputs = [\"How do you feel today?\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "input_ids = tokenizer(sample_inputs, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate responses\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "sample_outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, output in enumerate(sample_outputs):\n",
    "    print(f\"Input: {sample_inputs[i]}\")\n",
    "    print(f\"Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297b70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60d603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c005b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccf76e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def invoke(input_0: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initial exploration of the dataset\n",
    "    output = input_0.head()  # Display the first few rows of the dataframe\n",
    "    return output\n",
    "\n",
    "# Assuming input_0 is already provided as a DataFrame\n",
    "# Example usage:\n",
    "# output = invoke(input_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad770f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My order hasn't arrived yet.</td>\n",
       "      <td>We apologize for the inconvenience. Can you pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I received a damaged product.</td>\n",
       "      <td>We apologize for the inconvenience. Can you pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to return an item.</td>\n",
       "      <td>Certainly. Please provide your order number an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to change my shipping address.</td>\n",
       "      <td>No problem. Can you please provide your order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have a question about my bill.</td>\n",
       "      <td>We'd be happy to help. Can you please provide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   query  \\\n",
       "0           My order hasn't arrived yet.   \n",
       "1          I received a damaged product.   \n",
       "2              I need to return an item.   \n",
       "3  I want to change my shipping address.   \n",
       "4       I have a question about my bill.   \n",
       "\n",
       "                                            response  \n",
       "0  We apologize for the inconvenience. Can you pl...  \n",
       "1  We apologize for the inconvenience. Can you pl...  \n",
       "2  Certainly. Please provide your order number an...  \n",
       "3  No problem. Can you please provide your order ...  \n",
       "4  We'd be happy to help. Can you please provide ...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41d3d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove non-alphabetic characters and tokenize\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def invoke(explored_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    explored_data['query'] = explored_data['query'].apply(preprocess_text)\n",
    "    explored_data['response'] = explored_data['response'].apply(preprocess_text)\n",
    "    output = explored_data\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f28fa0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explored_data = invoke(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ea8d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def invoke(input_0: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    input_0: pd.DataFrame  data.csv\n",
    "    '''\n",
    "    # Inspect the first few rows of the dataframe\n",
    "    print(input_0.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = input_0.isnull().sum()\n",
    "    print(\"Missing values:\\n\", missing_values)\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    input_0 = input_0.dropna()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    input_0.columns = ['Query', 'Response']\n",
    "    \n",
    "    # Return the processed dataframe\n",
    "    output = input_0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60816a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          query  \\\n",
      "0          order hasn t arrived   \n",
      "1      received damaged product   \n",
      "2              need return item   \n",
      "3  want change shipping address   \n",
      "4                      question   \n",
      "\n",
      "                                            response  \n",
      "0  apologize inconvenience provide order number i...  \n",
      "1  apologize inconvenience provide photo damaged ...  \n",
      "2  certainly provide order number reason return p...  \n",
      "3  problem provide order number new shipping addr...  \n",
      "4  d happy help provide account number brief desc...  \n",
      "Missing values:\n",
      " query       0\n",
      "response    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order hasn t arrived</td>\n",
       "      <td>apologize inconvenience provide order number i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>received damaged product</td>\n",
       "      <td>apologize inconvenience provide photo damaged ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>need return item</td>\n",
       "      <td>certainly provide order number reason return p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want change shipping address</td>\n",
       "      <td>problem provide order number new shipping addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question</td>\n",
       "      <td>d happy help provide account number brief desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>schedule consultation appointment</td>\n",
       "      <td>d happy help provide phone number service inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>copy receipt</td>\n",
       "      <td>certainly provide order number account email l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>use competitor s coupon store</td>\n",
       "      <td>cases accept competitor coupons provide compet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>recycling program</td>\n",
       "      <td>yes recycling program provide email address se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>report lost stolen gift card</td>\n",
       "      <td>sorry hear provide gift card number available ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Query  \\\n",
       "0                order hasn t arrived   \n",
       "1            received damaged product   \n",
       "2                    need return item   \n",
       "3        want change shipping address   \n",
       "4                            question   \n",
       "..                                ...   \n",
       "69  schedule consultation appointment   \n",
       "70                       copy receipt   \n",
       "71      use competitor s coupon store   \n",
       "72                  recycling program   \n",
       "73       report lost stolen gift card   \n",
       "\n",
       "                                             Response  \n",
       "0   apologize inconvenience provide order number i...  \n",
       "1   apologize inconvenience provide photo damaged ...  \n",
       "2   certainly provide order number reason return p...  \n",
       "3   problem provide order number new shipping addr...  \n",
       "4   d happy help provide account number brief desc...  \n",
       "..                                                ...  \n",
       "69  d happy help provide phone number service inte...  \n",
       "70  certainly provide order number account email l...  \n",
       "71  cases accept competitor coupons provide compet...  \n",
       "72  yes recycling program provide email address se...  \n",
       "73  sorry hear provide gift card number available ...  \n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88827cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def invoke(inspected_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop rows with any null values in 'query' or 'response' columns\n",
    "    inspected_data = inspected_data.dropna(subset=['query', 'response'])\n",
    "    \n",
    "    # Apply text cleaning to 'query' and 'response' columns\n",
    "    inspected_data['query'] = inspected_data['query'].apply(clean_text)\n",
    "    inspected_data['response'] = inspected_data['response'].apply(clean_text)\n",
    "    \n",
    "    # Assign the cleaned DataFrame to the output variable\n",
    "    output = inspected_data\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7905852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspected_data = invoke(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc680cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "240bb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    padded_sequences = np.full((len(sequences), maxlen), value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if padding == 'post':\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "        elif padding == 'pre':\n",
    "            padded_sequences[i, -len(seq):] = seq\n",
    "    return padded_sequences\n",
    "\n",
    "def invoke(tokenized_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure 'query' and 'response' columns exist\n",
    "    if 'query' not in tokenized_data.columns or 'response' not in tokenized_data.columns:\n",
    "        raise KeyError(\"The DataFrame must contain 'query' and 'response' columns.\")\n",
    "    \n",
    "    # Convert 'query' and 'response' columns to lists of tokenized sequences\n",
    "    tokenized_data['query'] = tokenized_data['query'].apply(lambda x: list(map(int, x.split())))\n",
    "    tokenized_data['response'] = tokenized_data['response'].apply(lambda x: list(map(int, x.split())))\n",
    "    \n",
    "    # Determine the maximum length of sequences in both columns\n",
    "    max_len_query = max(tokenized_data['query'].apply(len))\n",
    "    max_len_response = max(tokenized_data['response'].apply(len))\n",
    "    max_len = max(max_len_query, max_len_response)\n",
    "    \n",
    "    # Pad sequences in 'query' and 'response' columns\n",
    "    tokenized_data['query'] = list(pad_sequences(tokenized_data['query'], max_len))\n",
    "    tokenized_data['response'] = list(pad_sequences(tokenized_data['response'], max_len))\n",
    "    \n",
    "    # Assign the processed DataFrame to the output variable\n",
    "    output = tokenized_data\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d102bfb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(tokenized_data)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe DataFrame must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert 'query' and 'response' columns to lists of tokenized sequences\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, x\u001b[38;5;241m.\u001b[39msplit())))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Determine the maximum length of sequences in both columns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36minvoke.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe DataFrame must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert 'query' and 'response' columns to lists of tokenized sequences\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, x\u001b[38;5;241m.\u001b[39msplit())))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Determine the maximum length of sequences in both columns\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'order'"
     ]
    }
   ],
   "source": [
    "invoke(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd90c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('customer_queries.csv')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Respond to the question: {query}\" for query in examples['query']]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"query\", \"response\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_test_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "eval_dataset = train_test_dataset['test']\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Trainer initialization\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(query):\n",
    "    input_text = f\"Respond to the question: {query}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model with some queries\n",
    "sample_queries = [\"How do you feel today?\", \"What's the time?\", \"Can you tell me something interesting?\"]\n",
    "\n",
    "for query in sample_queries:\n",
    "    response = generate_response(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
