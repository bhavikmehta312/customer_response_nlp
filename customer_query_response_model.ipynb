{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa89f3c0",
   "metadata": {},
   "source": [
    "# Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "58816961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gpt3_tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984bb4b",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b2ac78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: c790515a-08a0-42f4-bb26-b369c02adfce)')' thrown while requesting GET https://huggingface.co/datasets/Kaludi/Customer-Support-Responses/resolve/main/Customer-Support.csv\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "file_path = 'hf://datasets/Kaludi/Customer-Support-Responses/Customer-Support.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49acd89",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e568a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0                       My order hasn't arrived yet.   \n",
      "1                      I received a damaged product.   \n",
      "2                          I need to return an item.   \n",
      "3              I want to change my shipping address.   \n",
      "4                   I have a question about my bill.   \n",
      "..                                               ...   \n",
      "69  How do I schedule a consultation or appointment?   \n",
      "70                   Can I get a copy of my receipt?   \n",
      "71    Can I use a competitor's coupon at your store?   \n",
      "72                  Do you have a recycling program?   \n",
      "73       How do I report a lost or stolen gift card?   \n",
      "\n",
      "                                             response  \n",
      "0   We apologize for the inconvenience. Can you pl...  \n",
      "1   We apologize for the inconvenience. Can you pl...  \n",
      "2   Certainly. Please provide your order number an...  \n",
      "3   No problem. Can you please provide your order ...  \n",
      "4   We'd be happy to help. Can you please provide ...  \n",
      "..                                                ...  \n",
      "69  We'd be happy to help. Can you please provide ...  \n",
      "70  Certainly. Can you please provide your order n...  \n",
      "71  In some cases, we may accept competitor coupon...  \n",
      "72  Yes, we do have a recycling program. Can you p...  \n",
      "73  We're sorry to hear that. Can you please provi...  \n",
      "\n",
      "[74 rows x 2 columns]\n",
      "[15496, 11, 703, 389, 345, 30]\n"
     ]
    }
   ],
   "source": [
    "# Display some basic information about the dataset\n",
    "print(df)\n",
    "\n",
    "# Define a function to tokenize the dialogues\n",
    "def gpt3_tokenize(text):\n",
    "    tokens = gpt3_tokenizer.encode(text)\n",
    "    return tokens  # Return token IDs\n",
    "\n",
    "# Test the tokenizer function\n",
    "example_text = \"Hello, how are you?\"\n",
    "tokens = gpt3_tokenize(example_text)\n",
    "print(tokens)\n",
    "\n",
    "text = \" \".join(review for review in df['response'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "# Save the preprocessed responses as a JSON file\n",
    "with open('data/response_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)\n",
    "\n",
    "text = \" \".join(review for review in df['query'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "# Save the preprocessed query as a JSON file\n",
    "with open('data/query_token.json', 'a') as json_file:\n",
    "    json.dump(tokens, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574bb33",
   "metadata": {},
   "source": [
    "# Data Preprocessing and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2cfbda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['query'] = df['query'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)\n",
    "\n",
    "# Adding special tokens for clarity\n",
    "df['text'] = df['query'] + \" <|endoftext|> \" + df['response']\n",
    "\n",
    "# training and validation dataset, keeping 20% validation dataset\n",
    "train_df, val_df = train_test_split(df[['text']], test_size=0.2, random_state=42)\n",
    "train_texts = train_df['text'].tolist()\n",
    "val_texts = val_df['text'].tolist()\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a4ce6",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8b27269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 06:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.721300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.374900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=2.0498246399561566, metrics={'train_runtime': 409.2503, 'train_samples_per_second': 0.721, 'train_steps_per_second': 0.367, 'total_flos': 19270287360000.0, 'train_loss': 2.0498246399561566, 'epoch': 5.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = CustomerDataset(train_encodings)\n",
    "val_dataset = CustomerDataset(val_encodings)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f480f",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7db6b2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.4876656532287598\n",
      "Query: i have a question about my bill.\n",
      "Response:   we'd be happy to help. can you please provide your product name, sku, or zip code so we can check the availability?\n",
      "\n",
      "Query: can i get a replacement part for my product?\n",
      "Response:   we'd be happy to help. can you please provide the product name or sku so we can determine the best fit for you?\n",
      "\n",
      "Query: can i place a bulk order?\n",
      "Response:   we can't guarantee that your order will be processed or shipped. can you please provide the product name or sku so we can send you an estimate?\n",
      "\n",
      "Query: my order hasn't arrived yet.\n",
      "Response:   we apologize for the inconvenience. can you please provide the product name, sku, or sku so we can send you instructions on how to proceed?\n",
      "\n",
      "Query: i can't log into my account.\n",
      "Response:   we apologize for the inconvenience. can you please provide your email address so we can send you an update?\n",
      "\n",
      "Query: How do I report a problem with your website?\n",
      "Response:   we'd be happy to help. can you please provide the problem with your email?\n",
      "\n",
      "Query: I am not able to access your website\n",
      "Response:   we apologize for the inconvenience. can you please provide your email address so we can send you an email when we can provide you with an alternative?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss']))}\")\n",
    "\n",
    "# Function to generate responses with attention mask and adjusted parameters\n",
    "def generate_response(model, tokenizer, query, max_length=128, max_new_tokens=50, temperature=0.7, top_k=50, top_p=0.9, do_sample=True):\n",
    "    inputs = tokenizer.encode_plus(query + \" <|endoftext|>\", return_tensors='pt', padding='max_length', max_length=max_length, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split('<|endoftext|>')[-1].strip().replace(query,\"\")\n",
    "\n",
    "# Generate responses for some sample queries\n",
    "sample_queries = [\n",
    "    \"i have a question about my bill.\",\n",
    "    \"can i get a replacement part for my product?\",\n",
    "    \"can i place a bulk order?\",\n",
    "    \"my order hasn't arrived yet.\",\n",
    "    \"i can't log into my account.\",\n",
    "    \"How do I report a problem with your website?\",\n",
    "    \"I am not able to access your website\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    print(f\"Query: {query}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76891f3",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb9ae2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f580b2f121ef490e8d9cd057a7018f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Query:', layout=Layout(height='100px', width='100%'), placeholder='Type your q…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd7cf5358084931884ae9f390990c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Get Response', icon='check', style=ButtonStyle(), tooltip='Click to g…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0b81c587ad460a818d2af6b071548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='100px', width='100%'), placeholder='Response will appe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Jupyter notebook cell to create a simple demo\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the function to handle the query and generate response\n",
    "def on_button_click(b):\n",
    "    query = query_input.value\n",
    "    response = generate_response(model, tokenizer, query)\n",
    "    response_output.value = f\"Response: {response}\"\n",
    "\n",
    "# Create input and output widgets\n",
    "query_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your query here',\n",
    "    description='Query:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "response_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Response will appear here',\n",
    "    description='',\n",
    "    disabled=True,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a button widget\n",
    "button = widgets.Button(\n",
    "    description='Get Response',\n",
    "    disabled=False,\n",
    "    button_style='info',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to get response',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Assign the button click event\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(query_input, button, response_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23cc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
